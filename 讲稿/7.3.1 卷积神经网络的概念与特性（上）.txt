第3个知识点卷积神经网络的概念与特性（上），
P38,同学们大家好，这节课，我们开始学习卷积神经网络。学习卷积神经网络的第一步，需要学习卷积神经网络的概念与特性。卷积神经网络是包含有多个网络层的模型结构，在卷积神经网络中，不仅有全连接网络层，还有卷积神经网络层。全连接层和卷积层的根本区别有两个：第一，全连接层从输入特征空间中学到的是全局模式，卷积层学到的是局部模式。第二，对于图像来说，学到的就是在输入图像的二维小窗口中，发现的模式。卷积神经网络通过充分利用局部相关性和权值共享的思想，大大地减少了网络的参数量，从而提高了训练效率，更容易实现超大规模的深层网络。
P39,卷积神经网络具有平移不变性和空间层次结构。具有平移不变性是指，视觉世界的物体具有平移不变性，卷积神经网络在局部学到物体的某个特征之后，它可以在任何地方识别这个特征。对于全连接网络来说，如果该特征出现在新的位置，它只能重新学习。这个特性，使卷积神经网络在识别图像时，可以高效利用数据，只需要更少的训练样本，就可以学到具有较强泛化能力网络模型。具有空间层次结构是指，视觉世界的物体具有空间层次结构，通过卷积神经网络就可以，将超局部的边缘特征，组合成局部的物体特征，如图中，猫的耳朵、眼睛、鼻子等局部特征，最后汇聚为猫的整体特征。机器识别猫时，也像人们一样，通过局部特征的汇聚，来识别物体。
在卷积神经网络中，数据输入到达第一个卷积层，得到较小的局部特征，到达第二个卷积层，将由第一层特征汇聚了更多的特征，以此类推。这使得卷积神经网络抽取得到更复杂、深层的视觉特征，从而达到识别物体的目标。
P40,下面我们学习特征图的概念。在卷积神经网络中，一般输入层的输入特征图，是人能识别的，但是机器不能识别的，最后一层输出的输出特征图，是机器能识别的，但人不能识别的。卷积神经网络的目的，就是将输入的人能识别的特征图，转换为机器能识别的特征图，从而使用机器帮助人类，更轻松的工作、生活。我们在之前的课中都学过，彩色图片是由RGB图像组合而成的，深度轴的维度等于3，对应3个颜色通道，红、绿、蓝。在手写字符数据集中，为了减少计算量，使用的是黑白图像，深度轴的维度等于1。输出特征图是包含高度空间轴、宽度空间轴和一个深度轴，的3D张量的卷积输出，其深度与卷积核深度有关。
P41,如图，为输入特征图和输出特征图的示意图，这里不是最外层的和最后一层输出的特征图，是一个中间的一般形态。
P42,在卷积神经网络的内部，卷积的过程中，包含多个输入特征图和输出特征图。每个卷积层可以包括多个卷积核，每个卷积核，也可认为是一个过滤器，输出特征图，也可以称为响应图。输出特征图可看作是输入特征图在这个过滤器上的响应。
P43,例如，图中，输入特征图为手写字符0的的图片，经过卷积核后，得到一个如图的输出特征图。
P44,通过前面的学习，我们可以理解什么是特征图。在卷积神经网络中，一层中具有多个卷积核，这些卷积核对该层的输入特征，进行卷积，也就是进行特征提取，得到一个具有一定深度的特征图，该输出特征图的深度，是等于卷积核的数量。
P45,如图，输入为一个彩色图像，是具有3个通道特征图，经过一个深度为3的卷积核卷积，得到一个深度为1的输出特征图。也就是说，不论输入特征的深度多少，经过1个卷积核，得到的特征图的深度就是1。其计算过程为，卷积核每一层对应一个图像的通道，各自卷积计算后，得到的结果对应相加，再加上偏置误差项b，最后得到该位置的1个特征值。对应的该图为，红色通道的与卷积核的第一层，卷积后，得到值1，加上，绿色通道的与卷积核的第二层，卷积后，得到的值5，加上，蓝色通道的与卷积核的第三层，卷积后，得到的值-1，最后加上偏置误差项1，得到结果6，作为该位置的卷积运算特征值。本节课，我们就学习到这里，再见。

第3个知识点（下），P46,同学们大家好，我们接着学习卷积神经网络的概念与特性。这里我们先学习卷积的流程。流程的第一步，在3D的输入特征图上，滑动这些3×3或5×5的窗口，在每个可能的位置停止并提取周围特征的3D图块。
P47,流程的第二步，每个3D图块与卷积核卷积后，转换为形状为的1D向量，以3个卷积核为例，输出的深度D为3。
P48，流程的第三步，对所有这些向量进行空间重组，使其转换为，形状为的3D输出特征图，以3个卷积核为例，输出的深度为3，输出图的宽度和高度，与输入图的大小，填充方式，卷积核步长有关，这个在前面的卷积计算中已经讲过。
P49，输出特征图中的每个空间位置，都对应于输入特征图中的相同位置，例如输出特征图的左下角，包含了输入特征图左下角的信息。
P50，下面我们比较学习下，单通道与多通道输入输出的区别。首先我们回顾下之前学习的一个模型。也就是单输入、单输出的卷积模型，输入特征图，深度为1，卷积核深度也为1，卷积核个数为1。这样输出的特征图深度也为1，因为卷积核的个数为1。输出特征图的高度和宽度与输入特征图的大小，填充方式，卷积步长有关。以图为例，输入特征图为5*5，卷积核为1个，大小为3*3，深度为1，所以输出特征图为3*3，深度为1。
P51，在多输入通道单输出卷积运算中。需要注意以下几点：第一，在多通道输入的情况下，卷积核的通道数需要，与输入特征图通道数量相匹配，在代码实现中是自动匹配的，不需要额外设置。第二，卷积核的第i通道，与输入特征图第i通道运算，得到第i个中间矩阵，计算方法可认为是，单通道输入与单卷积核的运算。第三，所有通道的中间矩阵，对应元素再次相加，作为最终输出特征值。
P52，以该图为例。输入特征图有3个通道，卷积核深度需要匹配为深度为3。输入特征图的通道1，与卷积的第1层卷积运算后，得到结果为7，然后输入特征图的通道2，与卷积的第2层卷积运算后，得到结果为-11，接着输入特征图的通道3，与卷积的第3层卷积运算后，得到结果为-1，将3个结果相加，得到结果-5，存到输入矩阵的1行1列，作为输出特征值。同理可以得到其他位置的输出特征值。
P53，多输入通道、多输出通道，卷积的情况下，我们通过一组动图来学习。这里输入特征图为5*5，通道为3的彩色图片，采用0填充的方式，将输入特征图填充为7*7，通道为3的彩色图片，分别为图中的紫色矩阵。有两个卷积核W0和W1，卷积核深度自动匹配为3，每个卷积核后，还各有1个偏置误差参数b0和b1，两个卷积核分别为图中的红色矩阵。动图中，卷积步长为2，所以输出的特征图大小为3*3，输出的特征图深度为2，分别图中的2个绿色矩阵。卷积核卷积移动的过程，请大家注意看下对应的连线。（此处，需要停顿等动图走完）
P54，在上面多输入通道、多输出通道，卷积运算学习中，我们看到了步长和填充在2维卷积中的应用，这里我们再单独的学习下。如图，输入为一个5*5的特征图，卷积核为3*3，这里设步长为2，在卷积核对应卷积完成后，平移2格，然后再进行卷积。以图中为例，完成虚线框数值与卷积核卷积，得到值-5，记录到，输出矩阵的第2行1列中，然后平移2格，实线框数值与卷积核卷积，得到值12，记录到，输出矩阵的第2行2列中。
P55，填充的目的是在网络模型设计时，希望输出特征图高、宽，能够与输入特征图高、宽相同，方便网络参数的设计、残差连接等。在步长s=1时，经过卷积运算后，输出特征图的高、宽，会小于输入特征图的高宽，在s大于1时，输出特征图的高宽，必然会小于输入特征图的高和宽。所以，通过在原输入特征图高和宽维度上，进行填充若干无效元素操作，得到增大的输入特征图，在步长s=1时，使输出特征图高宽，与输入特征图高宽相同。
P56,池化是为了减少特征数量，俗称降维。图中，把蓝色框中的4个值，减少为1个值，就完成了降维。
P57,池化包含两种操作，第一种，最大值池化，就是选取一个规定大小窗口中，的数值最大的那个，作为输出的特征值，主要用来提取图片纹理。刚我们见到的池化就是最大池化。
P58,第二种，均值池化，就是选取一个规定大小窗口中，的数值的平均值，作为输出的特征图，主要用来保留背景特征。
P59,最大池化和卷积都是卷积神经网络中，必要的网络层，他们的区别和作用是什么呢？首先，最大池化是从输入特征图中提取窗口，并输出每个通道的最大值。最大池化使用求最大值，对局部图块进行变换；而卷积核是使用学到张量运算的线性变换，对局部图块进行变换。
P60,在网络模型设计中，最大池化的窗口一般为2*2，池化步长为2，其目的是将特征图下采样2倍；卷积的窗口一般为3*3，卷积步长为1。
P61,为什么要使用池化层，原因有两个，第一，是减少需要处理的特征图的元素个数。第二，是让卷积层的观察窗口越来越大，从而引入空间过滤器的层级结构。本节课我们就学习到这里，再见。

第4个知识点，P63,同学们大家好，这节课，我们学习卷积神经网络的实现。首先我们先学习下，卷积神经网络的结构，常见的卷积神经网络主要包含3大部分，第一是卷积层，第二是池化层，第三是全连接层。卷积层和池化层是为了对输入图像进行特征提取。全连接层，是把提取出来的特征组合连接为特征向量。本节，我们主要学习卷积层和池化层。
P64,如果把卷积层和池化层进行细化，可以包含如下几部分，首先是卷积运算，紧接着是批标准化处理，然后是使用激活函数增强模型的非线性表达能力，然后经过池化层，减少特征数量，为了提高模型的泛化能力，在神经网络的某次训练过程中，随机的，将一部分神经元按照一定概率，从神经网络中暂时舍弃，再次使用时，被舍弃的神经元恢复链接。最后经过全连接层，对提取出的特征进行全局学习，组成输入特征图的输出特征图。
P65,我们把刚才图中的各个部分，再逐个学习下。卷积，是使用卷积核对输入特征进行提取。批标准化，是对一小批数据在网络各层的输出做标准化处理，也就是把数据缩放到均值为0，标准差1的区间内。批标准化操作通常位于卷积层之后，激活层之前。
P66,激活,是通过激活函数，增强模型的非线性表达能力。池化，是通过池化运算，减少特征数量，降低数据维度，提高模型的运算速度。
P67,舍弃，是在神经网络的某次训练过程中，随机的，将一部分神经元按照一定概率，从神经网络中暂时舍弃，再次使用时，被舍弃的神经元恢复链接。这样某次训练过程中，参数总量减少同时，也可以防止模型的过拟合。最后就是全连接层，目的是对通过前面的各种网络层提取出的特征，进行全局学习。
P68,下面，我们学习卷积神经网络各层的实现代码。首先我们看卷积的代码实现，代码实现主要是运用keras框架的Conv2D函数完成的，函数中包含以下几个重要参数的设置，第一个是input_shape，是输入图片的高,宽和通道数，第二个是filters，卷积核的个数。第三个是strides，卷积步长,默认值为1。第四个是padding，填充参数，默认值为‘same’，表示全零填充。第五个是activation，激活函数的设置。
P69,批标准化代码实现，是运用keras框架的BatchNormalization（nomelaizishen）函数完成。
P70,激活代码实现，是运用keras框架的Activation函数完成，可以选择多个激活函数，例如relu，sigmoid激活函数等。
P71,池化代码实现，是运用keras框架的MaxPooling2D函数完成，这里用较常见的最大池化函数来讲解。最大池化，需要设置3个参数，第一个是池化核尺寸，第二是池化步长，第三个是填充参数，为0填充，或者不填充。例如，下面就是最大池化的代码实现，其中池化核大小为2*2，池化步长为none，默认为池化核大小2，填充使用全0填充。
P72,舍弃代码实现,是运用keras框架的Dropout函数完成，舍弃率在0和1之间浮动。例如，下面就是舍弃率为0.2的代码。
P73,全连接层代码实现，是运用keras框架的Flatten函数，该函数作用是将上层输出特征图展平，例如该程序，输入为28*28的图片，卷积核为64个，卷积核大小为3*3，使用0填充方式进行卷积，卷积输出特征图为，28*28大小，深度为64的数据，然后通过Flatten函数展平后，得到特征图为1维的28*28*64等于50176的数据。
P74,下面我们比较，在手写字符的识别项目中，分别使用全连接网络，和卷积神经网络的效果，验证下我们之前所学的知识。首先是全连接网络，输入为28*28的向量，全连接网络第一层有512个节点，输出层为10个节点，通过softmax函数，进行分类。
P75,通过训练，我们可以得到全连接网络的训练正确率为0.97879。
P76,我们设计一个卷积神经网络，该网络具有3个卷积层，2个池化层，2个全连接层。首先输入为28*28的向量，经过32个卷积核，卷积核大小为3*3，使用relu激活函数，的卷积层，进行特征提取，然后经过最大池化层的2*2池化核，池化，接着经过，64个卷积核，卷积核大小为3*3，使用relu激活函数的卷积层，进行特征提取，然后再经过最大池化层的2*2池化核，池化，接着经过64个卷积核，卷积核大小为3*3，使用relu激活函数的卷积层，进行特征提取，接着，把提取的特征图展平，展平后，经过节点为64和10的全连接网络层，最后经过softmax函数计算，得到的分类概率，完成手写字符的识别。
P77,通过训练，我们可以得到卷积神经网络的训练正确率为0.9909，卷积神经网络具有3个卷积层，2个池化层，2个全连接层，训练时间为1秒左右，全连接网络只有1个隐层，512个节点，训练时间约为1毫秒。通过不同网络模型比较，可以知道，卷积神经网络具有更深的隐层，可以得到更高的识别准确率。本节课，我们就学习到这里，再见。

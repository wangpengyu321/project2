知识点3：深度学习与神经网络概述
P11 深度学习是机器学习的一个分支领域：它是从数据中学习“表示”的一种新方法，强调从连续的层中进行学习，这些层对应于越来越有意义的表示。“深度学习”中的“深度”指的并不是利用这种方法所获取的更深层次的理解，而是指一系列连续的表示层。数据模型中包含多少层，这被称为模型的深度。这一领域的其他名称包括分层表示学习和层级表示学习。
现代深度学习通常包含数十个甚至上百个连续的表示层，这些表示层全都是从训练数据中自动学习的。与此相反，其他机器学习方法的重点往往是仅仅学习一两层的数据表示，因此有时也被称为浅层学习。 
P12 1943年，心理学家沃伦·麦卡洛克(Warren McCulloch)和数学家沃尔特·皮茨（Walter Pitts)根据生物神经元(Neuron)结构，提出了最早的神经元数学模型。该模型的输出f(x)=h(g(x))，其中g(x)等于对所有的xi求和，其中x的值域为0到1，模型通过g(x)完成输出值的预测，如图所示，当g(x)大于0时，输出为1，当g(x)小于等于0，输出为0，可以看出，MP神经元模型没有学习能力，智能完成固定逻辑的判定。
P13 1958年，美国心理学家弗兰克·罗森布莱特提出了第一个可以自动学习权重的神经元模型，称为感知机，输出值与真实值之间的误差用于调整神经元的权重参数{w1,...wn}。这种利用误差反馈修正权重的做法，被称之为“学习”。
P14美国心理学家弗兰克·罗森布莱特随后基于“Mark1感知机”硬件实现感知机模型，输入为400个单元的图像传感器，输出为 8个节点端子，可以成功识别一些英文字母。
P15 60年后，深度学习已经成为人工智能领域的主流技术，2018年，ACM决定将当年的图灵奖授予约书亚·本吉奥、杰弗里·辛顿和杨乐昆三位深度学习之父，以表彰他们给人工智能带来的重大突破，这些突破使深度神经网络成为计算的关键组成部分。本吉奥是蒙特利尔大学教授。辛顿是谷歌副总裁兼工程研究员、多伦多大学名誉教授。杨乐昆是纽约大学教授。
P16 2006 年，杰弗里·辛顿首次提出深度学习的概念。 2012 年，8层的深层神经网络AlexNet 发布，并在图片识别竞赛中取得了巨大的性能提升，此后数十层、 数百层、 甚至上千层的神经网络模型相继提出，展现出深层神经网络强大的学习能力。我们一般将利用深层神经网络实现的算法称作深度学习。 
P17 杨乐昆曾在多伦多大学跟随深度学习鼻祖进行博士后研究。在 20 世纪 80 年代末，杨乐昆 作为贝尔实验室的研究员提出了卷积网络技术，并展示如何使用它来大幅度提高手写识别能力，目前卷积神经网络已成为计算机视觉、语音识别等众多人工智能应用底层的基石。上世纪末本世纪初，当神经网络失宠时，杨乐昆是少数几名一直坚持的科学家之一。他在2021年8月出版的《科学之路：人，机器与未来》书中提到如果你真的相信一个概念，认为一个想法很有价值，就应该努力追求它，尽管可能要等到看到成绩后，人们才会支持这个想法。所以，很多想法是在技术成熟之前就诞生的。这里也希望同学你如果针对某些科学或者技术问题进行研究，并通过大量的调研认为方向正确，就应该像杨乐昆一样坚持自己的观点。
P18 下面我们来看看基于神经网络的深度学习模型是如何体现“深度”和“学习”的。前面提到过，深度学习中的分层是通过神经网络的模型来学习得到的，其结构是逐层堆叠。虽然深度学习的一些核心概念是从人们对大脑的理解中汲取部分灵感而形成的，但深度学习模型不是大脑模型。没有证据表明大脑的学习机制与现代深度学习模型是完全一致的，因为深度学习模型没有涉及到大脑活动中的生物学与化学等因素。深度学习是从数据中学习表示的一种数学框架。
P19总结一下前面的分析可以得出深度学习的技术定义是学习数据表示的多级方法。这个想法很简单，但事实证明，非常简单的机制如果具有足够大的规模，将会产生魔法般的效果。随着计算能力的提升和大数据时代的到来，高度并行化的GPU和海量数据让大规模神经网络的训练成为可能 。深度神经网络将数字图像转换成与原始图像差别越来越大的表示，而其中关于最终结果的信息却越来越丰富。你可以将深度网络看作多级信息蒸馏操作：信息穿过连续的过滤器，其纯度越来越高，得到的信息表示的形式对任务的帮助越来越大。

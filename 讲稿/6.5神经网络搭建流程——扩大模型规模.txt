文件：6.5神经网络搭建流程——扩大模型规模
P1、2：同学们大家好，本节课，我们针对神经网络搭建的流程，继续学习扩大模型规模这一部分内容。
P3：我们按照前述流程步骤的相关方法搭建并训练了神经网络模型，那么下面就会遇到这样的问题，也就是你所搭建的神经网络模型是否足够强大？它是否具有足够多的层和参数来对问题进行建模？例如，只有单个隐藏层且只有两个节点单元的网络，在MNIST手写数字识别问题上具有统计功效，但并不足以很好地解决问题。因此，可以说，///机器学习中无处不在的对立是优化和泛化的对立。那什么是优化呢？优化就是从训练的角度，我们的模型能够尽可能的拟合我们的训练数据，也就是比如说，大家在做作业题的时候，希望大家都能尽量的做到100分，做不到的，那就再做一遍，直到做好为止。这就是优化。那我们的目标是什么呢？就是在大家前所未见的考试中，或者说是在任一考试中，能够取得好成绩，这就称为泛化的能力。体现到我们的神经网络中，就是在训练好的模型上，我们使用验证集去进行模型的验证，这一验证的过程就是一种泛化能力的体现。在这里面，其实也存在着一定程度的优化与泛化之间的博弈。///那么，理想的模型就是刚好在欠拟合和过拟合的界线上，这里，欠拟合就是训练不足，过拟合就是训练过头了。而学习的状态就是在欠拟合和过拟合的界线上，换言之，就是在容量不足和容量过大的界线上。
所以，这里网络模型的规模，或者是模型的容量，作为影响因素，能够影响到拟合的效果，还有一个影响因素，就是训练的轮数。也就是同样的网络规模，你训练的轮数过多，就会过拟合，训练不够，也会欠拟合。下面，我们结合网络模型规模的不同，来看一下，模型学习的效果。我们对网络训练的轮数设置为固定的40轮，对隐藏层则设置三种不同节点数量规模的网络模型，///首先，当节点数量设置为16时，大家可以看一下，横坐标代表训练的轮数为40轮，左边这张图，纵坐标Loss代表训练的损失函数的实时变化值，蓝色圆点Training Loss，代表训练损失值，通过图中，可以看出，随着训练轮数的增多，Training Loss是不断的下降，同样，看右图，纵坐标Accuracy代表准确率，随着训练轮数的增加，也是逐渐上升并向1靠近。这就是优化。而验证时什么呢，就是验证集没有参与模型的训练，但当模型训练好之后，我们用验证集对模型进行验证，也就是看一下训练好的模型在从没见过的验证集数据上识别的准不准呢？这时，我们用蓝色的线段Validation来表示。大家想一下，最好的状态是什么？是不是两条曲线完全重合啊。但曲线的完全重合只是一种理想状态，那么为什么不能完全重合呢，就是在训练过程中，会有一个最优点或者是一个界线，过了这个点，就是过拟合。一旦过拟合了，就表现为在训练集上过度的优化，并拟合了过多的训练集上的细节，但在验证数据集上识别的性能却表现不佳，而这种表现不佳就体现在损失误差增加了，准确率下降了。我们还来看这个图，在16个节点的网络规模下，只训练40轮，虽然训练和验证的曲线稍稍分开了一点点，但其趋势整体还保持一致，并没有很明显的出现过拟合的拐点。
P4：然后，我们将网络的规模设置为第二种情况，96个节点，训练轮数仍然是40轮，当我们扩大了网络规模为96的时候，我们来看一下，什么时候是最佳训练轮数，很明显，是不是15或者说是15左右，是最佳训练轮数啊。///我们标出红色曲线来看一下，当Loss曲线在第15轮的时候，Validation Loss的这个点就是最低点，也就是在这个点之后，其Loss就上翘了，也就是这个地方就是拐点。再来看一下精确率曲线，Validation accuracy曲线，我们看，其趋势刚开始是一直上升，到这个点达到了最高点，或者说第一次达到了最高点，但其后面却怎么也无法更高了，那么我们就可以认为15轮就是我们隐藏层在96个节点的网络规模时的一个最优的训练轮数。
P5：好，我们进一步来看，当我们将网络的规模设置为第三种情况，1000个节点的时候，在训练轮数为5轮左右时，///我们用标出的红色曲线来看，Validation Loss在5轮处的这个点达到了拐点，而Validation accuracy也在5轮处的点大致达到了拐点。
这就是我们通过扩大网络规模的方式，来寻找网络训练最优值的一种方法。以上就是我们所说的扩大模型规模方面的相关内容。好了，同学们，今天的课就介绍到这里，谢谢大家。
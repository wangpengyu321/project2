“神经网络基础”讲稿
P2同学你好，今天我们来学习本门课程的第三单元内容《神经网络基础》，

P3本单元主要内容包括：神经网络设计过程，典型的神经网络审计案例，张量与张量运算这三部分。
P4通过本单元的学习，希望你能掌握神经网络设计过程、熟悉神经网络的搭建步骤与基本模块、掌握张量与张量运算。
P5在第一讲中，我们就提到了基于神经网络的深度学习，神经网络中的每个节点被称为神经元，就是模拟实际脑神经元的结构特征而形成的数学模型。（单击动画）同样，与大脑的神经网络类似，当多个神经元模型以分层的形式连接，就形成了人工神经网络。

P6我们用人工神经网络模拟了人类大脑的基本结构，目的是希望人工神经网络也能具有人类大脑的推理功能。人类大脑的推理功能受哪些因素影响呢？我们都知道，人类从新生儿到成年，大脑是在不断发育的，体现在脑神经细胞的增加以及细胞之间的连接复杂度，也就是节点多、节点间的连接多。那么是不是到成年之后，我们的大脑发育成熟，就自动具备了强大的推理能力、丰富的知识与经验了呢？（单击动画）显然，还差了一个条件，那就是外界刺激，即信息的涌入。大量的外部信息通过视觉、听觉涌入大脑，使我们大脑的神经网络连接产生了变化，根据信息的不同类型，大脑得了不同的训练与强化，体现在部分神经元之间的连接加强了，部分弱化了，是我们获得了特定的知识和技能，人工神经网络的模型构建和训练正是模拟了大脑发育成长与认知的过程。

P7结合刚才的分析，我们来看看神经网络的设计过程。首先，我们需要针对特定的应用设计我们的神经网络模型，在后面的课程里大家可以看到，对特定应用有针对性的模型设计，会提升模型训练与应用的效果。然后，要准备历史数据，数据量越大越好。（单击动画）将这些数据及对应的标签输入搭建好的神经网络模型，不断反复的进行参数优化，直到模型的功能指标达到要求，（单击动画）我们将这个过程称之为训练。这个过程可以用一个大家都理解的并经历的过的事情进行对比，我们每位同学都参加过高考，在高考前，大家肯定使用很多的试题进行过多次的模拟测试，这些试题就是历史数据，而模拟测试就是训练，目的是让你掌握某一科或者某一类型题目的解题思路。那么是不是此时的你就完成任务了呢？显然没有，我们做那多的模拟测试，不是为了把历史试卷答正确，而是希望在前所未见的高考考试中发挥出色，尽量答对所有的试题。（单击动画）也就是我们希望我们的网络模型不仅能在历史数据中有很高的指标，也能够在新数据上得出很好的结果，（单击动画）这个过程我们称为之推理。推理就是神经网络把从训练中学习到的能力应用到实际中。
这个图给大家展示就是神经网络的两个核心流程，训练与推理的关系。我们在后面的学习中，需要关注的有“数据的形式”，“模型的设计”，“训练原理”以及“结果评价”。

P8推理与训练的联系与区别
	推理是模型应用的目标；训练是模型应用效果的保证；
	推理和训练都有前向传导过程，但只有训练有反向传播过程；
	训练关注的是模型执行结果的误差；推理关注的是模型执行的结论；
	训练通常用的是历史数据（标注好的）；推理通常用的是新数据（未标注）；
	利用神经网络进行推理时，并不需要其训练时那样的海量资源。
	训练好了一个模型，在训练数据集中表现良好，但是我们的期望是它可以对以前没看过的图片进行识别。

P9在机器学习中，分类问题中的某个类别叫作类（class）。数据叫作样本（sample）。某个样本对应的类叫作标签（label）。以右图为例，我们找到了3类动物（猫、狗、鸟）的图片数据集，每个图片都是一个样本，而每个图片对应的类别标识为其对应的标签。

P10有了前面的概念和理论基础，我们来总结一下神经网络的设计过程：首先，采集大量的“样本-标签”数据；然后，根据应用需求搭建神经网络结构；接着，利用采集到的训练样本数据输入神经网络，进行参数优化，以获得最佳参数，参数优化的过程通常也是“反向传播”过程；在完成参数的优化后，我们最终的目标是将模型开展实际应用，即输入新的数据，模型输出分类或者预测结果。

P1下面我们来利用一个经典的神经网络设计案例来说明上述的设计过程。
我们希望构建一个神经网络模型，能够输入植物的特征属性，反馈植物的分类。例如对鸢尾花进行分类。为了能够简化模型输入特征的维度，我们收集了每朵鸢尾花的一些测量数据：花瓣的长度和宽度以及花萼的长度和宽度，通过比较不同品种鸢尾花鸢尾花的这些特征数据，可以得出鸢尾花的类别。于是，就可以通过采集大量特征数据对构成数据集。数据集包含鸢尾花花萼长、花萼宽、花瓣长、花瓣宽及对应的类别。其中前 4 个属性作为输入特征，类别作为标签， 0 代表狗尾草鸢尾， 1 代表杂色鸢尾， 2 代表弗吉尼亚鸢尾。把数据集喂入搭建好的神经网络结构，训练网络优化参数得到模型，模型读入新输入特征，输出识别结果。


P2下面我们使用一个简单的神经网络模型来介绍神经网络的推理过程。我们的目标是输入一个鸢尾花的特征，输出这个鸢尾花所属的种类。如图给出的神经网络只有两层，一个输入层一个输出层，其中输入层有4个节点，输出层有3个节点（这里请大家思考下，为什么输入层是4个节点，输出层是3个节点呢？）对了，请大家结合输入输出的特征考虑，因为我们的鸢尾花特征元素有4个，而输出是鸢尾花的种类，有3种，所以问题的数据维度就定义了神经网络输入输出的维度。
我们给出的网络节点采用的是MP神经元模型（单击动画），该模型是对所有输入因子进行线性求和，则该网络的数学表达式为y=x*w+b（单击动画），注意其中y、x、w、b均为向量。

P3下面我们来看下这个神经网络如何通过输入鸢尾花的特征，给出鸢尾花种类。请大家看下左边这幅示意图，神经网络的第一层的4个节点与输出层的3个节点之间的关系是全连接的关系，我们也称这种网络为全连接网络。我们来看下所有输入的特征{x0-x3}对于输出y0的影响，构成了一个线性方程y0=x0*w0,0+x1*w0,1+x2*w0,2+b0，将[w00,w01,w02]称为权重，b0称为偏置(线性方程中出现偏置b0，则灵活性大大增加，拟合能力增强），而每个输出都能够形成一个这样的线性方程，共三个y0，y1，y2，对应的权重w构成一个权重矩阵W，对应的偏置构成一个偏置向量B。我们的训练的目标就是找到一组最优的W和B,使我们的模型分类准确。
那么同学可能有疑问，在训练之前，W和B的值是如何确定的呢？答案是“随机确定的”，这种随机确定的方法，虽然在一开始，对于输出数据的分类是不准确的，但通过设计合理的训练方法，能够使权重W和偏置B逐步向最优值逼近。接下来，我们取出训练集中的某个样本数据，将其4个特征输入这个初始模型进行分类判断，看看模型是如何运行的。

P4我们去掉网络符号，用参数向量对输入数据运算来观察模型是如何实现对输入和输出数据之间的表示转换的。
输入样本的四个特征形成一个有4个元素的向量x，与随机初始化的权重矩阵W相乘，加上随机初始化的偏置向量B,得到一个有3个元素的输出向量y,y的三个元素值分别为[1.01，2.01，-0.66]，这是什么含义呢？从前面的输出节点的约定中，可以看出这3个值分别代表三类鸢尾花分类的判断得分，得分越高，说明这个样本为该类的可能性越高。于是，可以看出，针对这个样本，模型给出的判断是第1类鸢尾花分类，而我们的实际标签是第0类鸢尾花。问题来了，模型判断不准。为什么0类鸢尾的得分不是最高呢？这样容易理解，因为影响输出判断的参数都是随机生成的，就好像你如果完全没有学过英语，不认识任何英语单词，那你看到一个英语单词的时候，你的对这个单词的理解很可能就是错误的。这时我们应该怎么办呢？



P5还记得前几讲中我们介绍网络模型中的损失函数么？这时候就该发挥它的作用了，我们要训练网络，就需要衡量训练数据的标签，即标准答案y_与预测值y之间的差距，训练的目标就是寻找方法优化网络参数W,B，减少这种差距。损失函数可以定量判断W、 b的优劣， 当损失函数输出最小时，参数W、 b会出现最优值。所以，我们说损失函数是W和b函数。
损失函数的定义有多种方法，均方误差就是一种常用的损失函数，它计算每个前向传播输出 y 和标准答案y 的差求平方再求和再除以n 求平均值，表征了网络前向传播推理结果和标准答案之间的差距。

P6那么问题又来了，优化损失函数误差的方法是什么呢？这里我们来介绍神经网络训练重要的方法——梯度下降法。
首先回到我们的目标：找到一组参数w和b，使得损失函数最小。这是求函数极小值的问题，通常的方法就是将函数对各参数求偏导后，形成该函数的梯度向量，函数梯度下降方向是函数减小方向。梯度下降法的定义就是沿损失函数梯度下降的方向，寻找损失函数的最小值，得到最优参数的方法。参数更新的公式如图所示，这里xxx表示的就是损失函数对参数w的梯度，xxx表示损失函数对参数b的梯度，可以看出这是一个迭代的过程。这里lr表示什么呢？

P7 Lr参数的含义是学习率，或者理解为梯度下降的步长。给大家举个例子，我们下山时会选择路径，当然是希望寻找方向正确的路径，即最短的路径，通常大家的感受是一旦找到最短路径方向，是不是我们的更新参数的步子越大越好呢？这可不一定哦，对步长的选择是有一个折中的，这里引入一个概念叫“收敛”，表示我们逼近最优解的过程。请大家看图，当学习率设置的过小时，收敛过程将变得十分缓慢。但有些损失函数有可能在梯度下降的路径上不止有一个极值点，当学习率设置的过大时，梯度可能会在最小值附近来回震荡，甚至可能无法收敛，或者陷于局部最优，真不是我们希望得到的结果。所以设置合适的学习率，是会影响到算法的效率和效果的。在启发类算法中，选择步长非常重要，虽然不是本门课程介绍的重点，但希望大家能理解这个参数的含义。

P8大家通过前面的介绍，可以看出，更新网络参数是在神经网络中从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数的过程，我们称之为“反向传播”过程。下面我们用一个例子来看看反向传播更新参数的过程。
设某个损失函数为(w +1)2 ，则其对w的偏导数为2w+2 。权重更新公式为,设w在初始化时被随机初始化为5，学习率设置为0.2。则可按上述公式对 w 进行更新：第一次参数为w=5，按上式计算即5 - 0.2×(2×5 + 2)=2.6，第二次参数w=2.6, 更新后得到2.6 - 0.2 * (2 * 2.6 + 2) = 1.16，依次计算，我们最终能够当loss为最小值0时，w=-1，这实际上可以从这个损失函数的图像上较为直观的得出，在图中函数极值点上，w=-1。
在实际网络计算中，损失函数可能远比这个函数复杂，而且w通常为向量，所以往往无法从函数图形上直观看出函数极值点，这就需要我们用梯度下降法，逐渐收敛找出函数极值点。
P9 





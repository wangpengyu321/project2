P1同学们大家好，我是大数据技术课程团队的赵凯老师，这节课，我们讲什么是卷积？卷积，也叫褶积，是分析数学中一种重要的运算。在信号处理或图像处理中，经常使用一维或二维卷积，图像处理的二维卷积，是我们本门课的重点。
P2首先，我们先从字面意思，简单理解下卷积。在离散信号处理中，我们可以简单的认为，卷积的“卷”是指平移操作，“积”是指累加运算。为了更好的理解二维卷积，我们先从一维卷积讲起。在图中，输入的是一维的1行9列的序列，他的值为[1,1,-1,1,1,1,-1,1,1]，卷积核是一个一维的1行3列的序列，他的值为[1，-2，1]。卷积核的前3位，依次与输入序列的前3位对齐，然后相乘，得到将结果相加，也就是1×1+1×(－2)+(－1)×1=－2，然后将结果填写到输出序列的第一位中，完成卷积的第一位计算。紧接着是卷积核向左移动1位，然后按照对齐、相乘、求和的步骤，得到卷积的第二个结果4。以此类推，得到输出序列的值。从图中可以看到，卷积计算时，卷积核每位分别是红色的线，绿色的线和蓝色的线，最后与输入序列相乘后，求和汇聚到输出序列，每个颜色的线都在由左向右，一次平移1位。
P3学会了一维卷积，那么二维卷积基本计算方法跟一维卷积是一样的，同样是对齐相乘，结果相加，然后平移，如此重复。只不过我们输入的序列由一行多列，变为多行多列。在左边的公式中，中括号中，表示f卷积g,后面小括号内的值，表示卷积核与输入的对齐的位置。
P4我们以计算输出的第1行，第1到3列的值为例，讲解下二维卷积的计算过程。首先卷积核的1行1列与输入矩阵的1行1列对齐，然后对应数值分别相乘，接着将结果相加，记录在输出矩阵的1行1列。此时卷积核与输入矩阵相乘的位置记为小括号（-1，-1），因为，我们认为输入矩阵的中心位置为小括号（0，0）。相对于图中输入f(i,j)与g(m,n)卷积来说，第一步，输入f(i,j)的绿框中的感受野与g(m,n)卷积，计算的过程是1*（-1）+（-1）*1+0*2+（-1）*1+（-2）*（-1）+2*3+1*0+2*（-1）+（-2）*（-2）=7，然后将结果7，记录到输出矩阵的1行1列中。
P5我们讲完最左上端的感受野与卷积核的卷积后，我们需要将绿色的感受野的区域向右平移一位，然后在用同样的方法计算感受野与卷积和的卷积结果，作为输出，记录到输出矩阵的1行2列位置。经过对齐相乘，然后相加，得到结果为10，记录到输出矩阵的1行2列中。
P6紧接着，我们看图，自己计算一下，检查下计算结果与我的相同么？（等待几秒），是的，我的结果是3，将结果记录到输出矩阵的1行3列中，我们使用的方法是，对齐相乘，然后相加，计算后将感受野的区域平移1位。此时我们卷积核平移到最右端的怎么办呢？
P7我们需要将感受野的绿色框体重新回到最左端，然后先下平移1位，接着继续使用，对齐相乘，然后相加的卷积计算方法，得到结果-1。到这里我们已经学会了，基本的卷积计算的方法。
P8最后，给大家展示下输入f(i,j)与g(m,n)卷积的最后结果。大家看到结果之后，发现，输出和输入的数据，行、列的数量不一样，有些是位置是空的，是不是输入的数据的最边上的数据特征没有提取全？是的，确实存在这样的问题，那么我们将如何解决呢？下面我们讲下卷积的变种。
P9卷积的变种有两个方向，一个是将卷积输出量变得更少，一个是将卷积输出变得与输入一样多。同样，我们为了理解2维卷积的变种，我们先讲1维卷积的变种，后面小节，我们将讲解2维卷积变种应用，我们可以先简单认识一下。之前我们将卷积都是平移1位，那么可以平移2位么，平移以后会出现什么样的结果？这里我们输入1维序列[1,1,2,-1,1,-3,1],卷积核为[1,0,-1],那么我们先计算输出的第一位，计算方法与之前的相同，对齐相乘，然后相加，得到结果-1，记录在输出序列的第一位，此时我们将卷积核平移2位，也就是红线、绿线和蓝线，都同时向右平移2位，通过卷积计算方法得到结果1，记录到输出序列的第2位。同样方法，可以得到输出序列为[-1,1,0]。这就是，步长为2的情况下进行卷积。这就是通过调整步长，是输出量变得更少。这里掌握一个名词，步长，表示平移的位数。
P10接着我们讲卷积的第2种变种，目的是为了使输出序列与输入序列位数一样多。我们将序列的最左端和最右端填充一个数，一般情况是填充0，然后再进行卷积运算。如图，我们将输入1行7列的1维序列[1,1,2,-1,1,-3,1]，左右两端进行零填充后，序列变成1行9列的1维序列[0，1,1,2,-1,1,-3,1，0]，然后再与卷积核[1,0,-1]，进行卷积运算，我们设步长为1，计算方法与之前学的一样，我们可以得到输出序列为[-1,-1,2,1,2,0,-3]，是1行7列的，与输入序列时是一样的。那么大家课后可以尝试下，将刚才的2维数据，周边进行零填充，然后进行卷积，看看输出与输入的数据行列是不是相同的。
P11我们对二维离散卷积做一个小结。输出值与输入值之间建立的连接是卷积运算，卷积核仿照是生物学上感受野，引入的一个K*K的区域，然后每次通过移动卷积核，并与图片对齐位置相乘然后累加，得到此位置的输出值。
P12在卷积运算过程中，每个节点与输入节点，通过权值共享，从左上方逐步向右、向下移动卷积核，通过卷积运算提取每个位置上的像素特征，直至最右下方，最后得到图像的某些特征。
P13我们通过两张图，总结下上面所说的话。第一，有一个3*3的卷积核，对输入的5*5的矩阵数据进行卷积，得到一个3*3的卷积结果，也就5*5矩阵的特征值。卷积计算的过程是对齐相乘，然后相加，接着平移，如此反复，完成卷积运算。
P14我们前面总说通过卷积能提取图像的特征，那么对图像进行卷积后，能达到什么样的效果呢。如图，输入为一个玩具熊的图像，我们使用第一个卷积核，相当对图像没有进行任何处理，还是原图像。如果我们使用第二个卷积核，原图像就进行了锐化，就是颜色深的更深，浅的更浅，对比度增加了。如果我们使用第三个卷积核，原图像就进行了模糊处理，就是颜色深的变的浅了些，浅的变的深了些，对比度降低了。如果我们使用第四个卷积核，原图像就进行了边缘提取处理，就是只显示了颜色变换比较大的边，相邻像素值一致的，全部变成了黑色。特别是最后一个，通过图像的边缘特征提取，我们可以更好的识别图像，这就是图像的一种特征值。
P15最后，我们通过卷积的计算，来比较下卷积与全连接运算的参数变化。全连接的参数数量，就是输入和输出中所有连线的数量，每一条连线就表示一个参数。卷积通过权值共享，我们这里以卷积列数为3举例，那么这三条红线、绿线和蓝线，被重复的运用了，所以参数的减少量是十分可观的。那么，2维的图像数据，参数变化也是一样，我们这里就不再赘述了。卷积网络的参数，相比全连接网络的参数是减少的。这节课就讲到这里，同学们再见。

P37,同学们大家好，我是大数据技术课程团队的赵凯老师，这节课，我们开始讲卷积神经网络？在讲卷积神经网络第第一步需要讲卷积神经网络的概念与特性。卷积神经网络是包含有多个网络层的模型结构，在卷积神经网络中，不仅有全连接网络层，还有卷积神经网络层，全连接层和卷积层的根本区别有如下两个：第一，全连接层从输入特征空间中学到的是全局模式，卷积层学到的是局部模式。第二，对于图像来说，学到的就是在输入图像的二维小窗口中发现的模式。卷积神经网络通过充分利用局部相关性和权值共享的思想，大大地减少了网络的参数量， 从而提高训练效率，更容易实现超大规模的深层网络。

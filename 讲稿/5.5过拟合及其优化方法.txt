P2 同学好，本讲将介绍模型的优化方法。机器学习的根本问题是优化和泛化的对立，\\\其中优化指调节模型以在训练数据上得到最佳性能（即机器学习中的学习）；\\\而泛化是指训练好的模型在前所未见的数据上的性能好坏。
P3 我们用一张图来展示优化与泛化的关系，图在纵轴表示误差，横轴为模型的容量，可以理解为模型的复杂程度，图中蓝色虚线表示模型在训练集上的表现，称之为泛化误差，绿色实线表示模型在测试集上的表现，称之为训练误差。从图中可以看出，优化和泛化是相关的，\\\开始时，模型的容量较小，数据表示能力有限，训练误差越小，泛化误差也越小。这时的模型是欠拟合的，即仍有改进的空间，模型开始学习仅和训练数据有关的模式。\\\随着不断扩大模型容量，在某个容量指标下，训练误差和泛化误差都达到最小值，此时模型表现能力最优，称之为较好拟合。\\\随着模型容量继续扩大，表示能力趋于复杂和强大，训练误差仍继续减小，但泛化误差反而随着模型容量变大而变大，此时我们称模型开始过拟合，模型学习到了过多训练数据的模式细节，但在测试数据上的泛化能力减弱。
P4下面我们来对学习过程的这3个状态进行一个小结。欠拟合：模型不能够很好地学习到训练集数据的模态，导致训练集上表现不佳，同时在未见的样本上表现也不佳。\\\好比大家备考，光看书不做题觉得自己会了，上了考场却正确率很低。
P5而过拟合指模型在训练集上面表现较好，但是在未见的样本上表现不佳，即模型泛化能力偏弱。\\\就像某位同学备考时做课后题全都能做对，上了考场正确率很低；\\\真正好的拟合是模型具有较好地泛化能力。\\\即练习题全能做对，考试正确率也很高。
P6 对于欠拟合，通常的做法就是加大网络规模和加强训练的强度。但是如何避免过拟合呢？最优解决方法是获取更多的训练数据。\\\我们常说见多识广，在模型训练中，就是指模型的训练数据越多，越能代表真实数据分布，训练模型的泛化能力自然也越好。\\\如果无法获取更多的数据，次优的解决方法就是调节模型允许存储的信息量，\\\如果一个网络只能记住几个模式，优化过程会迫使模型集中学习最重要的模式，这样更可能得到良好的泛化。
P7 防止神经网络过拟合的常用方法包括：获取更多的训练数据、减小模型容量、添加权重正则化、添加 dropout。
P8 在数据集有限的情况下，防止过拟合的最简单的方法就是减小模型大小。减小模型大小即减少模型中可学习参数的个数，参数个数通常由层数和每层的单元个数决定。\\\在深度学习中，模型中可学习参数的个数又被称为模型的容量，指模型拟合复杂函数的能力。为了让损失最小化，网络必须学会对目标具有很强预测能力的压缩表示，这也正是我们感兴趣的数据表示。同时请记住，你使用的模型应该具有足够多的参数，以防欠拟合，即模型应避免记忆资源不足。在容量过大与容量不足之间要找到一个折中。你必须在验证集上评估一系列不同的网络架构，以便为数据找到最佳的模型大小。要找到合适的模型大小，一般的工作流程是开始时选择相对较少的层和参数，然后逐渐增加层的大小或增加新层，直到找到一个最优的模型网络结构。
P9 我来看看模型网络规模缩小时有什么变化。图例展示的是训练轮数与验证误差之间的关系，横坐标为训练轮数，纵坐标为验证误差。在一定网络的规模下，\\\当达到一定的训练轮数时，会得到最小的验证误差，这个验证误差就是对应当前网络规模的最小误差。随着训练轮数继续增加，模型学习到了训练数据模式的很多细节，造成在验证集上表现不佳，体现在验证误差上升。在图中圆点是更小网络的验证损失值，十字是原始网络的验证损失值，\\\可以看出更小的网络在6轮训练后开始过拟合，这个时间要晚于原始网络在3 轮后就出现过拟合。
P10 那如果我们扩大网络规模呢？请看图例种更大的网络与原始网络的性能对比。圆点是更大网络的验证损失值，十字是原始网络的验证损失值，\\\可以看出，更大的网络只过了一轮就开始过拟合，过拟合也更严重。其验证损失的波动也更大。 
P11 这幅图同时给出了这两个网络的训练损失。可以看出，更大网络的训练损失很快就接近于零。网络的容量越大，它拟合训练数据的速度就越快，但也更容易过拟合。 
P12 你可能知道奥卡姆剃刀原理：如果一件事情有两种解释，那么最可能正确的解释就是最简单的那个，即假设更少的那个。\\\这个原理也适用于神经网络学到的模型：给定一些训练数据和一种网络架构，很多组权重值都可以解释这些数据。而简单模型比复杂模型更不容易过拟合。这里简单模型是指在满足模型需要的容量的情况下，参数更少的模型。 
P13 一种常见的降低过拟合的方法就是强制让模型权重只能取较小的值，从而限制模型的复杂度，这使得权重值的分布更加规则。这种方法叫作权重正则化，其实现方法是向网络损失函数中添加与较大权重值相关的成本。\\\这个成本有两种形式。L1 正则化，指添加的成本与权重系数的绝对值成正比。L2 正则化指的是添加的成本与权重系数的平方成正比。
P14 在 Keras中，添加权重正则化的方法是向层传递权重正则化项实例作为关键字参数。代码示例中l2(0.001) 的意思是该层权重矩阵的每个系数都会使网络总损失增加 0.001 * weight。\\\注意，由于这个惩罚项只在训练时添加，所以这个网络的训练损失会比测试损失大很多，这样促使网络训练时参数分布更为合理。
P15 从图中可以看出，假设两个模型的参数个数相同，具有 L2正则化的模型（圆点）比原始模型（十字）更不容易过拟合。 
P16 dropout 是神经网络最有效也最常用的正则化方法之一，它是由多伦多大学的杰弗里辛顿教授和他的学生开发的。\\\对某一层使用dropout，就是在训练过程中随机将该层的一些输出特征舍弃，比如直接设置为0。\\\dropout 比率是被设为0的特征所占的比例，通常在0.2~0.5范围内。
P17 假设有一个包含某层输出的矩阵，其内容如图所示。训练时，我们随机将矩阵中一部分值，即50%的比例设为 0，由于少了一半的单元，在输出时乘以2进行成比例增大。这一方法可能看起来有些奇怪和随意。它为什么能够降低过拟合？ Hinton 说他的灵感之一来自于银行的防欺诈机制。用他自己的话来说：“我去银行办理业务。柜员不停地换人，于是我问其中一人这是为什么。他说他不知道，但他们经常换来换去。我猜想，银行工作人员要想成功欺诈银行，他们之间要互相合作才行。这让我意识到，在每个样本中随机删除不同的部分神经元，可以阻止它们的阴谋，因此可以降低过拟合。” 其核心思想是在层的输出值中引入噪声，打破不显著的偶然模式。如果没有噪声的话，网络将会记住这些偶然模式。
p18 从案例代码上可以看出，在Keras中，添加权重正则化的方法是向层传递权重正则化项实例作为关键字参数。代码中的参数配置Keras.layers.Dropout(0.5)就表示本层在训练时按照50%的概率丢弃一些节点。
P19 我们再次看到，添加Dropout规则的网络性能相比参考网络有明显提高。
本讲内容就到这里，谢谢
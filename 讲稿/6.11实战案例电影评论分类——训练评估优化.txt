文件：6.11神经网络实战案例：电影评论分类——训练评估优化
P1：同学们大家好，本节课，我们针对电影评论分类这一神经网络实战案例，继续讲解其训练评估优化等方面所涉及到的内容。
P2：好，我们首先来看一下选择评估指标以及确定评估方法。我们之前介绍过，///这里的电影评论分类是一个平衡二分类问题，也就是50%的正面好评，和50%的负面差评，那我们主要考虑他的什么指标啊，准确率吧。那选择什么样的评估方法呢？///我们采用留出法划分出验证集，也就是在原始训练数据中留出10000个样本作为验证集。具体而言，///（单击出现后面的代码）就是25000个训练样本集中，我们留出其前10000个样本数据作为验证集，那从10000到最后的总共15000个样本数据，我们将之用作于训练集。同样，对应的样本标签也是按这种划分方式进行处理。
P3：然后，就开始创建网络了。///首先，是要确定网络的输入输出，我们之前也讲过，输入是一个向量化的文本，而输出是不是就是一个概率值啊，这个概率值也代表着目标值等于1的可能性，即属于正面评论的可能性。好，下面我们就开始进一步地创建网络的层了。///第一层，大家看，总共16个节点，选用的是ReLu激活函数，输入的形状是10000维的一阶张量。为什么是10000维呢，是不是就是我们之前所讲的，每条评论10000维独热码One-hot编码的形式啊。然后，///下面再创建一层隐藏层，也是总共16个节点，激活函数是ReLu。///最后一层输出层，既然是一个二分类的概率值，那么就是1个节点，对应的激活函数也就是Sigmoid函数，
P4：然后，下面就该编译了。对模型进行编译的时候，我们给出了3种不同形式的编译方法。这三种编译方法的作用其实都是一样的，只不过第一种可以看做是傻瓜式的，后两种可以自行设置像学习率这样的参数。好，这就是编译。
P5：编译了之后，就要开始训练网络了。神经网络的训练，用的还是fit函数。参数里面填入的是训练数据、训练标签，以及训练轮数初始设置为20轮，batch_size为512，而且还有每一轮训练后的一次验证集数据的评估。
P6：然后，这就是我们的训练结果中的最后三轮的训练结果。可以看到，这里每一轮都有四个指标，也就是在训练集上的loss和准确率，以及在验证集上的loss和准确率。
P7：得到了这些参数指标，我们还可以对其进行进一步地分析，那么，我们就可以用history将其保存起来，也就是将这四个变量保存起来。
P8：通过上面的方法，我们将每一轮的参数指标都保存下来之后，就可以将他们给画出来。这就是所画出来的损失的图像，包括训练的损失和验证的损失。可以看出，两条曲线的走势完全不一样，那肯定就出现了过拟合了。那最佳的训练轮数是几呢？从图中可以看看，///（单击出现红色虚线）是不是在2.5和5之间啊。
P9：然后我们再看一下准确率的曲线，两条曲线的走势也不一样，出现了过拟合，///（单击出现红色虚线）而最佳的训练轮数也是出现在2.5和5之间。所以，我们这里的最佳训练轮数就可以认为是3轮或者4轮。
P10：好，我们假定最佳训练轮数是4轮，找到这个参数了，然后就可以重新生成一个新的网络。在这个新创建的网络中，我们可以看一下，与之前的那个网络的相关参数都一样，只有这里的fit训练的时候稍有不同吧。这是再训练的时候，我们用的就不是之前留出验证集之后的15000个训练集了，而是用的是所有的25000个训练集，也就是x_train，y_train，然后epoch设置为我们之前找到的最佳训练轮数。
P11：然后，我们就对这个网络再进行训练，我们看，是不是总共训练了4轮啊，注意，这里的4轮训练，我们就只有训练集了，就没有验证集了。所以，通过4轮的训练，我们这里的loss就是0.1682，准确率就是94.16%；那我们同时在25000个测试集上进行评估，得到的就是0.2935的loss，以及88.34%的准确率。这里，只是一种简单的网络结构和方法，得到的是88%的准确率，如果你更新他的网络结构，包括层数啊，参数啊，等等，你还能得到将近95%的准确率的。
P12：好，下面我们再在所训练好的网络上使用测试集x_test数据，进行predict的预测推理，网络就能够得到这样的一个输出的结果。在这样的结果中，每一个数都是一个概率值。那这里有多少个值呢，还记得吗？总共25000个。因为我们的测试集有25000条评论。那这个结果代表什么含义呢？我们来看一下，也就是25000条测试集中，其第一条评论，有20%的可能是正面评论；第二条评论则有99.99%的可能是正面评论；以此类推，这些数值都代表着其所对应的评论有多少的可能是正面的评论。这就是对测试集数据的一种预测。而我们的电影评论的分类也由此完成了。
好了，同学们，以上就是我们关于电影评论分类之训练评估优化的相关内容，我们的这个电影评论分类实战案例也就都给大家介绍完了。今天的课也就给大家讲到这里，谢谢大家。
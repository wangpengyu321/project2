P2 同学你好，本讲我们来介绍神经网络的优化方法。
P3 我们来看一个分类问题的案例，大家可以尝试下，是否能在图中找到一条直线作为分界线，将圆点和三角点的区域划分开？\\\似乎无论怎么画，都不能完全的将圆点和三角点分开，\\\必须采用曲线才能完成这个任务。
P4 我们的神经网络中的神经元感知机模型，就是一种线性模型，感知机的输入和输出是一种线性关系。线性模型是机器学习中间最简单的数学模型之一，参数量少， 计算简单， 但是只能表达线性关系。人类目前对于复杂大脑的感知和决策的研究尚处于初步探索阶段， 如果只使用一个简单的线性模型去逼近复杂的人脑图片识别模型，很显然不能胜任。
P5 是否采用多个线性节点搭建多层网络，就能提升模型的表达能力呢？请大家看图例，当搭建含有一个隐藏层的感知机模型时，\\\本质上，网络的输入和输出仍然时线性关系。并没有提升模型的表达能力。
P6 我们再来看一个曲线拟合的例子，通过观察图中的散点可以大致推测数据属于某2次多项式分布, 使用二次多项式函数模型去学习，则能学到比较合适的模型，。\\\如果我们用简单的线性函数去学习时，会发现很难学习到一个较好的函数，从而出现训练集和测试集表现都不理想的现象，这种现象叫做欠拟合。\\\但如果用较复杂的函数模型去学习时，表达能力过强时，有可能学习到的函数会过度地“拟合”训练集样本，从而导致在测试集上表现不佳, ，伤害模型的泛化能力。这种现象叫做欠拟合。
P8 目前我们所采用的多神经元模型仍是线性模型，表达能力偏弱，既然线性模型不可行，而复杂模型又容易过度表达，我们可以给线性模型嵌套一个非线性函数用delta表示，即可将其转换为非线性模型。 我们把这个非线性函数称为激活函数，有了这样的非线性激活函数以后, 神经网络的非线性表达能力加强了。
P9 ReLU函数是一种常用的激活函数，它非常简单，在y=x基础上面截去了x< 0的部分， 可以直观地理解为ReLU 函数仅保留正的输入部份，清零负的输入，具有单边抑制特性。虽然简单，ReLU 函数却有优良的非线性特性，而且梯度计算简单，训练稳定，是深度学习模型使用最广泛的激活函数之一。 
P10 sigmoid函数也是非常常用的激活函数，通常中间层使用 relu 作为激活函数，最后一层使用 sigmoid 激活，以输出一个 0~1 范围内的概率值 。sigmoid函数只能处理两个类，不适用于多分类问题，\\\而softmax函数可以有效解决这个问题，并且softmax函数大都运用在神经网路中的最后一层网络中，使得值得区间在（0,1）之间。为了能应对不同模型不同应用的需求，激活函数的种类非常多，大家感兴趣可以参阅相关资料，深入了解不同种类激活函数的特点。
本讲内容就到这里，谢谢

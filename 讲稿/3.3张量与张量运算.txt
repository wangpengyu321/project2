知识点3.3：张量与张量运算
P1 同学你好，本节我们来学习张量与张量运算。Tensorflow是谷歌公司在2015年9月开源的一个深度学习框架，是一个采用数据流图用于数值计算的开源软件库。它灵活的架构让你可以在多种平台上展开计算，例如台式计算机中的一个或多个CPU（或GPU），服务器，移动设备等等。TensorFlow 最初由谷歌大脑小组的研究员和工程师们开发出来，用于机器学习和深度神经网络方面的研究，这个系统的通用性使其也可广泛用于其他计算领域。\\\数据流图用“结点” 和“线” 的有向图来描述数学计算。“节点” 一般用来表示施加的数学操作，也可以表示数据输入的起点/输出的终点， “线”表示“节点”之间的输入/输出关系。这些数据“线”传输大小可动态调整的多维数组，即“张量”就是tensor。张量从图中流过的直观图像是这个工具取名为Tensorflow的原因。一旦输入端的所有张量准备好，节点将被分配到各种计算设备异步并行地执行运算。\\\Tensorflow随着神经网络技术的演进也在不断的更新，2020年1月，Tensorflow 2.1 发布。本门课程使用的就是Tensorflow 2.1 框架。
P2 我们首先看下张量的定义与形式。张量其实就是一个多维数组，其中张量的维数也称张量的阶数。比如0阶张量，也称之为标量，类似S=256这样变量就是0阶张量，一阶张量也称之为向量，是一维数组的形式，二阶张量也称为矩阵，也可以根据数据形式的需要定义更高阶的张量形式，\\\张量可以表示0阶到n阶的数组。从张量的形式可以看出，各种应用场景的数据格式，都可以转换为张量形式。
P3 如何创建一个张量呢？可以使用tensorflow中的constant方法实现，请看示例代码使用constant方法创建了一个具有两个元素的一阶张量。\\\这是这段代码的运行结果
P4 张量的数据类型与python语言的数据类型类似，可以有整形、浮点型、布尔型和字符串类型等多种形式。
P5 Tensorflow框架内置了很多创建多种形式和内容的张量函数，比如\\\创建为全零张量的zeros函数，\\\创建为全一张量的ones函数，\\\可以创建填充任意指定值的fill函数等，\\\这里需要说明的是在使用这些函数时，需要明确生成张量的维度，例如二维张量，可以用中括号、行、列形式表示这个张量的形状，其中行、列上的数字表示在生成的张量这个维度上的元素个数。\\\在示例代码中，我分别创建了一个全零矩阵，1阶全1列表和一个2阶填充9的张量。\\\大家可以对应代码的运行结果来理解函数的功能。
P6 在张量运算中，有时需要对张量元素的类型进行转换，或者计算张量维度上的最大最小值，这些常用函数在tensorflow中也有定义，\\\在示例代码中，创建了一个具有3个浮点型元素的一阶张量，利用cast函数将每个元素转换为整型，同时，利用reduce_min和reduce_max函数分别计算这个张量的最小值和最大值。
P7 大家可以根据结果对照代码理解函数的功能。
P8 在高阶张量中，有时会根据需要在不同维度上进行运算，这需要我们对张量的维度操作有较为清晰的理解。在一个二维张量或数组中，可以通过调整 axis参数 等于0或1 控制执行维度。以二阶张量为例，axis=0代表跨行操作，而axis=1代表跨列操作，如果不指定axis，则所有元素参与计算。\\\那么对于更高阶的张量，如何利用axis设置操作的维度呢？这里介绍一个小技巧：一个张量，维度的识别可以按照“展开”的概念来理解，最外层的括号表示的维度越低，最内层的括号表示维度越高，可以理解高层维度是底层维度元素的展开。按照这种方法，大家可以以图中这个三阶张量为例，axis=0表示在最外侧的维度进行操作，即红色中括号的这个维度上，axis=1表示在蓝色括号的维度上进行操作，axis=2示在绿色括号的维度上进行操作。这个方法可以扩展到n阶张量仍然有效。
P9 下面我们来看一个例子，\\\比如利用reduce_mean函数计算某个维度上的平均值，对于声明的二阶张量，当不显示赋值axis参数时，表示对所有维度的所有元素值求平均，计算方法为1+2+3+2+2+3=13，元素数量为6，则13除以6取证后为2。\\\而利用reduce_sum函数计算在axis=1这个维度上的平均值，根据刚才的方法，这个张量最高阶时2阶，所以我们axis=1就是在第二个维度上进行求和，则结果得到一个一阶张量[6，7]。
P10 Tensorflow还提供了丰富的张量四则混合运算函数，能够满足绝大多数情况下的张量运算要求，\\\需要注意的时只有维度相同的张量才可以做四则运算。
P11 下面要介绍的这个张量函数非常有使用价值，它时tensorflow框架中data.Dataset对象的from_tensor_slices方法，这个方法的功能是切分传入张量的第一维度，生成输入特征/标签对，构建数据集。\\\函数参数为输入特征和其对应的标签列表数据。Numpy和Tensor格式都可用该语句读入数据。
P12我们用一个代码案例来理解这个函数的作用，假设某个数据集有4个样本，样本特征为整型数值，分别为12,23,10,17，对应的标签也是整形数值，为0, 1, 1, 0，\\\我们将这特征列表和标签列表代入from_tensor_slices函数，通过打印数据集，可以看出函数切分了features和labels中的数值，构成一个（样本特征，标签）数据对，其中样本特征和标签都为标量。\\\大家可以对照看下运行的结果
P13 在很多机器学习任务中，特征并不总是连续值，而有可能是离散值。所以有些特征转化为数字表示后，上述数据也不能直接用在我们的分类器中。因为，分类器通常默认数据是连续且有序的。为了解决上述问题，其中一种可能的解决方法是采用独热编码。独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。比如1表示是，0表示非。\\\按照上述规则，以鸢尾花分类为例，不同类别的鸢尾花独热码标签如图所示。
P15 在机器学习尤其是深度学习中，softmax是个非常常用而且比较重要的函数，尤其在多分类的场景中使用广泛。他把一些输入映射为0-1之间的实数，并且归一化保证和为1，因此多分类的概率之和也刚好为1。\\\仍然是上讲种鸢尾花分类为例，输出层在没有经过任何激活函数处理而直接输出时，得到的预测值y为[1.01，2.01，-0.66]，\\\利用softmax函数的转换规则，则变成了[0.256，0.695，0.048]，即这个输出有25.6%的可能性为类别0，有69.5%的可能性为类别1，有4.8%的可能性为类别2,现在softmax函数的处理使得结果即容易理解，也实现了归一化操作。
P16 n分类的n个输出 （y0 ，y1, …… yn-1）通过softmax( ) 函数，也可以符合n个变量的概率分布。
以上就是对张量与张量运算的介绍，本讲内容就到这里。
